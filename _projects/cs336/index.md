---
layout: page
title: CS336 — Language Modeling from Scratch
description: Working through Stanford CS336 assignments — building a BPE tokenizer, Transformer, and AdamW optimizer from scratch in PyTorch.
importance: 3
category: work
---

## Overview

[Stanford CS336](https://github.com/stanford-cs336) is a graduate course on language modeling from scratch. The assignments cover every component of a modern LLM — from tokenization to the Transformer architecture to distributed training — implemented from first principles in PyTorch.

> **Status:** In progress. Assignment notes and solutions to be filled in.

## Assignments

| Assignment | Topic | Status |
|---|---|---|
| 1 | BPE Tokenizer + Transformer + AdamW | ⬜ TODO |
| 2 | Systems & Efficiency | ⬜ TODO |
| 3 | Alignment / RLHF | ⬜ TODO |

## What I Want to Build

- [ ] BPE tokenizer from scratch
- [ ] Multi-head attention (with RoPE)
- [ ] Transformer block (pre-norm, SwiGLU FFN)
- [ ] AdamW with weight decay and gradient clipping
- [ ] Mixed-precision training
- [ ] Evaluation: perplexity and sampling

## Links

- [Course GitHub](https://github.com/stanford-cs336)
- [Spring 2025 Assignment 1](https://github.com/stanford-cs336/spring2025-assignment1-basics)

## Notes

See [notes.md](notes) for implementation notes, and [resources.md](resources) for reference papers and links.
